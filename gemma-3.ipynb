{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce55ac5",
   "metadata": {},
   "source": [
    "# Softmax is not enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499ead3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\miniconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-1b-it\"\n",
    "# MODEL_NAME = \"google/gemma-3n-e2b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d3ef13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1020 13:00:03.911000 132796 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "def get_polynomial_value(x: float, c: list[float]) -> float:\n",
    "    cur_val = 0\n",
    "    for i in range(len(c) - 1):\n",
    "        cur_val = (cur_val + c[i]) * x\n",
    "    return cur_val + c[-1]\n",
    "\n",
    "POLY_FIT = torch.nn.Parameter(torch.tensor([-0.037, 0.481, -2.3, 4.917, -1.791]), requires_grad=True) # see Figure 6\n",
    "\n",
    "def softmax_adaptive_temperature(logits, dim, poly_fit=POLY_FIT, dtype=torch.float32):\n",
    "    \"\"\"\n",
    "    from \"Softmax is not Enough\" Figure 4, adapted to PyTorch\n",
    "    \"\"\"\n",
    "    original_probs = torch.softmax(logits, dim=dim, dtype=dtype)\n",
    "    entropy = torch.sum(-original_probs * torch.log(original_probs + 1e-9), axis=-1, keepdims=True) # compute the Shannon entropy\n",
    "    beta = torch.where(\n",
    "        entropy > 0.5,  # donâ€™t overcorrect low-entropy heads\n",
    "        torch.maximum(get_polynomial_value(entropy, poly_fit), torch.tensor(1.0)),  # never increase entropy\n",
    "        torch.tensor(1.0)\n",
    "    )\n",
    "    return torch.softmax(logits * beta, dim=dim, dtype=dtype)\n",
    "    \n",
    "class SoftmaxZero(nn.Module):\n",
    "    # for testing only\n",
    "    def __init__(self):\n",
    "        super(SoftmaxZero, self).__init__()\n",
    "\n",
    "    def forward(self, logits):\n",
    "        return torch.zeros_like(logits)\n",
    "    \n",
    "def adaptive_temperature_eager_attention_forward(\n",
    "    module: nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    dropout: float = 0.0,\n",
    "    scaling: Optional[float] = None,\n",
    "    softcap: Optional[float] = None,\n",
    "    **kwargs,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    A copy of gemma3's eager attention forward function, but using adaptive temperature softmax.\n",
    "    \"\"\"\n",
    "    if scaling is None:\n",
    "        scaling = module.head_dim**-0.5\n",
    "\n",
    "    # I don't like importing a foreign module like this, but ok for now\n",
    "    key_states = transformers.models.gemma3.modeling_gemma3.repeat_kv(key, module.num_key_value_groups)\n",
    "    value_states = transformers.models.gemma3.modeling_gemma3.repeat_kv(value, module.num_key_value_groups)\n",
    "\n",
    "    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n",
    "\n",
    "    if softcap is not None:\n",
    "        attn_weights = attn_weights / softcap\n",
    "        attn_weights = torch.tanh(attn_weights)\n",
    "        attn_weights = attn_weights * softcap\n",
    "    if attention_mask is not None:  # no matter the length, we just slice it\n",
    "        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "        attn_weights = attn_weights + causal_mask\n",
    "\n",
    "    # upcast attention to fp32\n",
    "    # attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n",
    "    attn_weights = softmax_adaptive_temperature(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n",
    "    \n",
    "    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n",
    "    attn_output = torch.matmul(attn_weights, value_states)\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    return attn_output, attn_weights\n",
    "\n",
    "# register the new attention function\n",
    "# this is so scrappy\n",
    "transformers.modeling_utils.ALL_ATTENTION_FUNCTIONS.register(\"adaptive_temperature_eager\", adaptive_temperature_eager_attention_forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3384f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\miniconda3\\envs\\nlp\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\alexg\\.cache\\huggingface\\hub\\models--google--gemma-3-1b-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 036c11ca-ba93-4563-bbaf-835edd794a33)')' thrown while requesting HEAD https://huggingface.co/google/gemma-3-1b-it/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 036c11ca-ba93-4563-bbaf-835edd794a33)')' thrown while requesting HEAD https://huggingface.co/google/gemma-3-1b-it/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "adaptive_model = transformers.Gemma3ForCausalLM.from_pretrained(MODEL_NAME, attn_implementation=\"adaptive_temperature_eager\")\n",
    "original_model = transformers.Gemma3ForCausalLM.from_pretrained(MODEL_NAME, attn_implementation=\"eager\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "adaptive_pipeline = transformers.pipeline(\"text-generation\", model=adaptive_model, tokenizer=tokenizer, device=\"cpu\")\n",
    "original_pipeline = transformers.pipeline(\"text-generation\", model=original_model, tokenizer=tokenizer, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dddf2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a poem about Hugging Face, aiming to capture its essence and impact:\n",
      "\n",
      "**The Neural Web**\n",
      "\n",
      "A whisper of code, a digital bloom,\n",
      "Hugging Face arises, dispelling gloom.\n",
      "A platform vast\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},]\n",
    "        },\n",
    "    ],\n",
    "]\n",
    "output = original_pipeline(messages, max_new_tokens=50)\n",
    "print(output[0][0]['generated_text'][-1]['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df5dc0d",
   "metadata": {},
   "source": [
    "# CLRS Dataset Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f04173e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import CLRS text utilities for generating algorithmic reasoning tasks\n",
    "import sys\n",
    "import os\n",
    "from datasets import IterableDataset, Value, Features\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), 'clrs-repo'))\n",
    "\n",
    "from clrs._src.clrs_text.huggingface_generators import clrs_generator\n",
    "\n",
    "# Create a simple configuration with specific algorithms and fixed lengths\n",
    "# This avoids the type error by using explicit numeric values\n",
    "algos_and_lengths = {\n",
    "    \"minimum\": [3, 6, 9, 12, 15]\n",
    "}\n",
    "\n",
    "ds = IterableDataset.from_generator(\n",
    "        clrs_generator,\n",
    "        features=Features(\n",
    "            {\n",
    "                \"text\": Value(dtype=\"string\", id=None),\n",
    "                \"question\": Value(dtype=\"string\", id=None),\n",
    "                \"answer\": Value(dtype=\"string\", id=None),\n",
    "                \"algo_name\": Value(dtype=\"string\", id=None),\n",
    "                \"length\": Value(dtype=\"int32\", id=None),\n",
    "                \"use_hints\": Value(dtype=\"bool_\", id=None),\n",
    "            }\n",
    "        ),\n",
    "        gen_kwargs={\n",
    "            \"algos_and_lengths\": algos_and_lengths,\n",
    "            # \"use_hints\": True\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9aab81c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model_on_clrs(pipeline, dataset, num_samples=10, max_new_tokens=100):\n",
    "    \"\"\"\n",
    "    Evaluate a model on CLRS algorithmic reasoning tasks\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Create generator and take first num_samples\n",
    "    sample_iter = iter(dataset)\n",
    "\n",
    "    for i in tqdm(range(num_samples)):\n",
    "        try:\n",
    "            sample = next(sample_iter)\n",
    "        except StopIteration:\n",
    "            break\n",
    "            \n",
    "        text = sample['text']\n",
    "        question = sample['question']\n",
    "        correct_answer = sample['answer']\n",
    "        algo_name = sample['algo_name']\n",
    "        hints_used = sample['use_hints']\n",
    "        \n",
    "        \n",
    "        # Generate model response\n",
    "        \n",
    "        prompt = [\n",
    "            [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": f\"You are a computer emulating minumum function. Find the smallest number in the array and ONLY GIVE its index, nothing else\"},]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [{\"type\": \"text\", \"text\": f\"{question}\"},]\n",
    "                },\n",
    "            ],\n",
    "        ]\n",
    "\n",
    "        output = pipeline(prompt, max_new_tokens=max_new_tokens)\n",
    "        model_answer = output[0][0]['generated_text'][-1]['content']\n",
    "        \n",
    "        # Simple exact match evaluation\n",
    "        is_correct = correct_answer.strip().lower() == model_answer.lower()\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        results.append({\n",
    "            \n",
    "            'algo_name': algo_name,\n",
    "            'question': question,\n",
    "            'correct_answer': correct_answer,\n",
    "            'model_answer': model_answer,\n",
    "            'is_correct': is_correct\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n--- Example {i+1} ({algo_name}) ---\")\n",
    "        # print(f\"Text: {text}...\")\n",
    "        print(f\"Question: {question}...\")\n",
    "        print(f\"Correct: {correct_answer}\")\n",
    "        print(f\"Model: {model_answer}...\")\n",
    "        # print(f\"Hints Used: {hints_used}\")\n",
    "        print(f\"Correct: {is_correct}\")\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(f\"\\nOverall Accuracy: {correct}/{total} = {accuracy:.3f}\")\n",
    "    \n",
    "    return results, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "248072e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BASELINE MODEL (standard softmax):\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]WARNING:absl:Ignoring kwargs {'use_padding'} when building sampler class <class 'clrs._src.samplers.SortingSampler'>\n",
      "WARNING:absl:Ignoring kwargs {'use_padding'} when building sampler class <class 'clrs._src.samplers.SortingSampler'>\n",
      "WARNING:absl:Ignoring kwargs {'use_padding'} when building sampler class <class 'clrs._src.samplers.SortingSampler'>\n",
      "WARNING:absl:Ignoring kwargs {'use_padding'} when building sampler class <class 'clrs._src.samplers.SortingSampler'>\n",
      "WARNING:absl:Ignoring kwargs {'use_padding'} when building sampler class <class 'clrs._src.samplers.SortingSampler'>\n",
      "WARNING:absl:Ignoring kwargs {'use_padding'} when building sampler class <class 'clrs._src.samplers.SortingSampler'>\n",
      "WARNING:absl:Ignoring kwargs {'use_padding'} when building sampler class <class 'clrs._src.samplers.SortingSampler'>\n",
      "WARNING:absl:Ignoring kwargs {'use_padding'} when building sampler class <class 'clrs._src.samplers.SortingSampler'>\n",
      "WARNING:absl:Ignoring kwargs {'use_padding'} when building sampler class <class 'clrs._src.samplers.SortingSampler'>\n",
      "WARNING:absl:Ignoring kwargs {'use_padding'} when building sampler class <class 'clrs._src.samplers.SortingSampler'>\n",
      "  5%|â–Œ         | 1/20 [00:01<00:29,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.548 0.715 0.602 0.544 0.423 0.645 0.437 0.891 0.963 0.383 0.791 0.528]\n",
      "min:\n",
      "...\n",
      "Correct: 9\n",
      "\n",
      "\n",
      "Model: 0\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 2/20 [00:03<00:28,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 2 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.568 0.925 0.071 0.087 0.02 0.832 0.778 0.87 0.978 0.799 0.461 0.78]\n",
      "min:\n",
      "...\n",
      "Correct: 4\n",
      "\n",
      "\n",
      "Model: 0\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–Œ        | 3/20 [00:04<00:27,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 3 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.548 0.715 0.602]\n",
      "min:\n",
      "...\n",
      "Correct: 0\n",
      "\n",
      "\n",
      "Model: 10\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 4/20 [00:06<00:27,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 4 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.548 0.715 0.602 0.544 0.423 0.645 0.437 0.891 0.963]\n",
      "min:\n",
      "...\n",
      "Correct: 4\n",
      "\n",
      "\n",
      "Model: 10\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:08<00:25,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 5 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.548 0.715 0.602 0.544 0.423 0.645 0.437 0.891 0.963 0.383 0.791 0.528 0.568 0.925 0.071]\n",
      "min:\n",
      "...\n",
      "Correct: 14\n",
      "\n",
      "\n",
      "Model: 0\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:09<00:23,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 6 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.118 0.639 0.143 0.944 0.521 0.414 0.264 0.774 0.456 0.568 0.018 0.617]\n",
      "min:\n",
      "...\n",
      "Correct: 10\n",
      "\n",
      "\n",
      "Model: 0\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:11<00:21,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 7 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.612 0.616 0.943 0.681 0.359 0.437 0.697 0.06 0.666 0.67 0.21 0.128]\n",
      "min:\n",
      "...\n",
      "Correct: 7\n",
      "\n",
      "\n",
      "Model: 0\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:13<00:20,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 8 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.383 0.791 0.528 0.568 0.925 0.071 0.087 0.02 0.832]\n",
      "min:\n",
      "...\n",
      "Correct: 7\n",
      "\n",
      "\n",
      "Model: 10\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:15<00:19,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 9 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.315 0.363 0.57 0.438 0.988 0.102 0.208 0.161 0.653 0.253 0.466 0.244]\n",
      "min:\n",
      "...\n",
      "Correct: 5\n",
      "\n",
      "\n",
      "Model: 0\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [00:16<00:17,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 10 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.778 0.87 0.978 0.799 0.461 0.78 0.118 0.639 0.143]\n",
      "min:\n",
      "...\n",
      "Correct: 6\n",
      "\n",
      "\n",
      "Model: 0\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [00:18<00:15,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 11 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.087 0.02 0.832 0.778 0.87 0.978 0.799 0.461 0.78 0.118 0.639 0.143 0.944 0.521 0.414]\n",
      "min:\n",
      "...\n",
      "Correct: 1\n",
      "\n",
      "\n",
      "Model: 0\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [00:20<00:13,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 12 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.548 0.715 0.602 0.544 0.423 0.645]\n",
      "min:\n",
      "...\n",
      "Correct: 4\n",
      "\n",
      "\n",
      "Model: 10\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [00:21<00:12,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 13 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.264 0.774 0.456 0.568 0.018 0.617 0.612 0.616 0.943 0.681 0.359 0.437 0.697 0.06 0.666]\n",
      "min:\n",
      "...\n",
      "Correct: 4\n",
      "\n",
      "\n",
      "Model: 0\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [00:23<00:10,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 14 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.437 0.891 0.963 0.383 0.791 0.528]\n",
      "min:\n",
      "...\n",
      "Correct: 3\n",
      "\n",
      "\n",
      "Model: 10\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [00:25<00:08,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 15 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.944 0.521 0.414 0.264 0.774 0.456 0.568 0.018 0.617]\n",
      "min:\n",
      "...\n",
      "Correct: 7\n",
      "\n",
      "\n",
      "Model: 0\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [00:27<00:06,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 16 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.568 0.925 0.071 0.087 0.02 0.832]\n",
      "min:\n",
      "...\n",
      "Correct: 4\n",
      "\n",
      "\n",
      "Model: 10\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [00:28<00:05,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 17 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.544 0.423 0.645]\n",
      "min:\n",
      "...\n",
      "Correct: 1\n",
      "\n",
      "\n",
      "Model: 10\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [00:30<00:03,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 18 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.67 0.21 0.128 0.315 0.363 0.57 0.438 0.988 0.102 0.208 0.161 0.653 0.253 0.466 0.244]\n",
      "min:\n",
      "...\n",
      "Correct: 8\n",
      "\n",
      "\n",
      "Model: 0\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [00:31<00:01,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 19 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.612 0.616 0.943 0.681 0.359 0.437 0.697 0.06 0.666]\n",
      "min:\n",
      "...\n",
      "Correct: 7\n",
      "\n",
      "\n",
      "Model: 0\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:32<00:00,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 20 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.158 0.11 0.656 0.138 0.196 0.368 0.82 0.097 0.837 0.096 0.976 0.468 0.976 0.604 0.739]\n",
      "min:\n",
      "...\n",
      "Correct: 9\n",
      "\n",
      "\n",
      "Model: 0\n",
      "...\n",
      "Correct: False\n",
      "\n",
      "Overall Accuracy: 0/20 = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate baseline model with standard softmax\n",
    "print(\"Evaluating BASELINE MODEL (standard softmax):\")\n",
    "print(\"=\" * 50)\n",
    "baseline_results, baseline_accuracy = evaluate_model_on_clrs(\n",
    "    original_pipeline, ds, num_samples=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "133860ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating ADAPTIVE TEMPERATURE MODEL:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]WARNING:absl:Ignoring kwargs {'use_padding'} when building sampler class <class 'clrs._src.samplers.SortingSampler'>\n",
      "WARNING:absl:Ignoring kwargs {'use_padding'} when building sampler class <class 'clrs._src.samplers.SortingSampler'>\n",
      "WARNING:absl:Ignoring kwargs {'use_padding'} when building sampler class <class 'clrs._src.samplers.SortingSampler'>\n",
      "WARNING:absl:Ignoring kwargs {'use_padding'} when building sampler class <class 'clrs._src.samplers.SortingSampler'>\n",
      "WARNING:absl:Ignoring kwargs {'use_padding'} when building sampler class <class 'clrs._src.samplers.SortingSampler'>\n",
      "WARNING:absl:Ignoring kwargs {'use_padding'} when building sampler class <class 'clrs._src.samplers.SortingSampler'>\n",
      "WARNING:absl:Ignoring kwargs {'use_padding'} when building sampler class <class 'clrs._src.samplers.SortingSampler'>\n",
      "WARNING:absl:Ignoring kwargs {'use_padding'} when building sampler class <class 'clrs._src.samplers.SortingSampler'>\n",
      "WARNING:absl:Ignoring kwargs {'use_padding'} when building sampler class <class 'clrs._src.samplers.SortingSampler'>\n",
      "WARNING:absl:Ignoring kwargs {'use_padding'} when building sampler class <class 'clrs._src.samplers.SortingSampler'>\n",
      "  5%|â–Œ         | 1/20 [00:05<01:39,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.548 0.715 0.602 0.544 0.423 0.645 0.437 0.891 0.963 0.383 0.791 0.528]\n",
      "min:\n",
      "...\n",
      "Correct: 9\n",
      "\n",
      "\n",
      "Model: ... 7\n",
      "\n",
      "*   **...** and then...\n",
      "\n",
      "*   **0**\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 2/20 [00:11<01:47,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 2 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.568 0.925 0.071 0.087 0.02 0.832 0.778 0.87 0.978 0.799 0.461 0.78]\n",
      "min:\n",
      "...\n",
      "Correct: 4\n",
      "\n",
      "\n",
      "Model: You're asking for me to write a whole paragraph. I'm ready for the next prompt!  Let's go.\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–Œ        | 3/20 [00:12<01:00,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 3 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.548 0.715 0.602]\n",
      "min:\n",
      "...\n",
      "Correct: 0\n",
      "\n",
      "\n",
      "Model: ...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 4/20 [00:30<02:27,  9.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 4 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.548 0.715 0.602 0.544 0.423 0.645 0.437 0.891 0.963]\n",
      "min:\n",
      "...\n",
      "Correct: 4\n",
      "\n",
      "\n",
      "Model: It's a bit of a scramble, but I'll try to answer your \"4323\" prompt.\n",
      "\n",
      "It's a bit of a scramble, but I'll try to answer your prompt.\n",
      "\n",
      "I'll try to answer your prompt.  It's a bit of a mess, but I'll try to provide a minimal answer:\n",
      "\n",
      "0\"\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–Œ       | 5/20 [00:38<02:12,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 5 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.548 0.715 0.602 0.544 0.423 0.645 0.437 0.891 0.963 0.383 0.791 0.528 0.568 0.925 0.071]\n",
      "min:\n",
      "...\n",
      "Correct: 14\n",
      "\n",
      "\n",
      "Model: 7\n",
      "\n",
      "Okay, so I'll give up on the'm' and start again.\n",
      "\n",
      "Here are 7 ways to continue the sequence:\n",
      "\n",
      "8. ...\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 6/20 [00:42<01:39,  7.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 6 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.118 0.639 0.143 0.944 0.521 0.414 0.264 0.774 0.456 0.568 0.018 0.617]\n",
      "min:\n",
      "...\n",
      "Correct: 10\n",
      "\n",
      "\n",
      "Model: 07)\n",
      "\n",
      "its. 7. 0\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 7/20 [00:50<01:39,  7.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 7 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.612 0.616 0.943 0.681 0.359 0.437 0.697 0.06 0.666 0.67 0.21 0.128]\n",
      "min:\n",
      "...\n",
      "Correct: 7\n",
      "\n",
      "\n",
      "Model: The response is ready.\n",
      "\n",
      "I'm sorry, but I can't provide a minimum number of characters.  Let me know if you would like me to help with that!\n",
      "\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 8/20 [00:52<01:08,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 8 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.383 0.791 0.528 0.568 0.925 0.071 0.087 0.02 0.832]\n",
      "min:\n",
      "...\n",
      "Correct: 7\n",
      "\n",
      "\n",
      "Model: 871.\n",
      "\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 9/20 [00:59<01:06,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 9 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.315 0.363 0.57 0.438 0.988 0.102 0.208 0.161 0.653 0.253 0.466 0.244]\n",
      "min:\n",
      "...\n",
      "Correct: 5\n",
      "\n",
      "\n",
      "Model: I'm sorry, I missed a key part of your request. I'm ready for the next part, please!...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [01:21<01:49, 10.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 10 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.778 0.87 0.978 0.799 0.461 0.78 0.118 0.639 0.143]\n",
      "min:\n",
      "...\n",
      "Correct: 6\n",
      "\n",
      "\n",
      "Model: ...min\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "Let's break down this number sequence:\n",
      "\n",
      "*   **0** -  A beginning\n",
      "*   **0** -  Zero is a placeholder.\n",
      "\n",
      "I apologize for my previous response. I need to help you understand the logic behind the number sequence. It's a \"do-nothing\" list - a list of tasks that are never completed. The goal is to create a list that makes sense of the prompt, and it's a bit...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 11/20 [01:24<01:18,  8.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 11 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.087 0.02 0.832 0.778 0.87 0.978 0.799 0.461 0.78 0.118 0.639 0.143 0.944 0.521 0.414]\n",
      "min:\n",
      "...\n",
      "Correct: 1\n",
      "\n",
      "\n",
      "Model: ...2.5. \n",
      "\n",
      "...3.14.\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 12/20 [01:27<00:54,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 12 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.548 0.715 0.602 0.544 0.423 0.645]\n",
      "min:\n",
      "...\n",
      "Correct: 4\n",
      "\n",
      "\n",
      "Model: min: 10\n",
      "\n",
      "...\n",
      "\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 13/20 [01:34<00:48,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 13 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.264 0.774 0.456 0.568 0.018 0.617 0.612 0.616 0.943 0.681 0.359 0.437 0.697 0.06 0.666]\n",
      "min:\n",
      "...\n",
      "Correct: 4\n",
      "\n",
      "\n",
      "Model: 86.  We are told that the number 86 is the smallest possible number.\n",
      "\n",
      "Therefore, you've provided an 86.\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 14/20 [01:36<00:33,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 14 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.437 0.891 0.963 0.383 0.791 0.528]\n",
      "min:\n",
      "...\n",
      "Correct: 3\n",
      "\n",
      "\n",
      "Model: 39\n",
      "97\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 15/20 [01:51<00:40,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 15 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.944 0.521 0.414 0.264 0.774 0.456 0.568 0.018 0.617]\n",
      "min:\n",
      "...\n",
      "Correct: 7\n",
      "\n",
      "\n",
      "Model: 57the 17th 17.777\n",
      "\n",
      "of.77\n",
      "\n",
      "58.  777.777\n",
      "\n",
      "It's difficult to continue the sequence after the given character.  Let me know if you'd like me to continue!\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 16/20 [01:59<00:32,  8.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 16 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.568 0.925 0.071 0.087 0.02 0.832]\n",
      "min:\n",
      "...\n",
      "Correct: 4\n",
      "\n",
      "\n",
      "Model: my\n",
      "\n",
      "It's a bit of a random string of words!\n",
      "\n",
      "Nothing\n",
      "\n",
      "Let me know if you'd like me to continue generating the array.  ðŸ˜Š\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 17/20 [02:03<00:20,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 17 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.544 0.423 0.645]\n",
      "min:\n",
      "...\n",
      "Correct: 1\n",
      "\n",
      "\n",
      "Model: â€¦123\n",
      "\n",
      "of\n",
      "\n",
      "code\n",
      "...123\n",
      "...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 18/20 [02:04<00:10,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 18 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.67 0.21 0.128 0.315 0.363 0.57 0.438 0.988 0.102 0.208 0.161 0.653 0.253 0.466 0.244]\n",
      "min:\n",
      "...\n",
      "Correct: 8\n",
      "\n",
      "\n",
      "Model: ...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 19/20 [02:04<00:03,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 19 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.612 0.616 0.943 0.681 0.359 0.437 0.697 0.06 0.666]\n",
      "min:\n",
      "...\n",
      "Correct: 7\n",
      "\n",
      "\n",
      "Model: ...\n",
      "Correct: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [02:10<00:00,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 20 (minimum) ---\n",
      "Question: minimum:\n",
      "key: [0.158 0.11 0.656 0.138 0.196 0.368 0.82 0.097 0.837 0.096 0.976 0.468 0.976 0.604 0.739]\n",
      "min:\n",
      "...\n",
      "Correct: 9\n",
      "\n",
      "\n",
      "Model: Let me know the number you want me to generate, and I'll do my best!...\n",
      "Correct: False\n",
      "\n",
      "Overall Accuracy: 0/20 = 0.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate adaptive temperature model\n",
    "print(\"\\nEvaluating ADAPTIVE TEMPERATURE MODEL:\")\n",
    "print(\"=\" * 50)\n",
    "adaptive_results, adaptive_accuracy = evaluate_model_on_clrs(\n",
    "    adaptive_pipeline, ds, num_samples=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b659dbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMPARISON RESULTS\n",
      "============================================================\n",
      "Baseline (standard softmax):     0.000\n",
      "Adaptive temperature softmax:    0.000\n",
      "Improvement:                     +0.000\n",
      "\n",
      "Per-algorithm performance:\n",
      "Algorithm            Baseline   Adaptive   Diff      \n",
      "--------------------------------------------------\n",
      "minimum              0.000      0.000      +0.000    \n"
     ]
    }
   ],
   "source": [
    "# Compare results\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARISON RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Baseline (standard softmax):     {baseline_accuracy:.3f}\")\n",
    "print(f\"Adaptive temperature softmax:    {adaptive_accuracy:.3f}\")\n",
    "print(f\"Improvement:                     {adaptive_accuracy - baseline_accuracy:+.3f}\")\n",
    "\n",
    "# Analyze by algorithm type\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_by_algorithm(results):\n",
    "    algo_stats = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "    \n",
    "    for result in results:\n",
    "        algo = result['algo_name']\n",
    "        algo_stats[algo]['total'] += 1\n",
    "        if result['is_correct']:\n",
    "            algo_stats[algo]['correct'] += 1\n",
    "    \n",
    "    return {algo: stats['correct']/stats['total'] if stats['total'] > 0 else 0 \n",
    "            for algo, stats in algo_stats.items()}\n",
    "\n",
    "baseline_by_algo = analyze_by_algorithm(baseline_results)\n",
    "adaptive_by_algo = analyze_by_algorithm(adaptive_results)\n",
    "\n",
    "print(f\"\\nPer-algorithm performance:\")\n",
    "print(f\"{'Algorithm':<20} {'Baseline':<10} {'Adaptive':<10} {'Diff':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "all_algos = set(baseline_by_algo.keys()) | set(adaptive_by_algo.keys())\n",
    "for algo in sorted(all_algos):\n",
    "    baseline_acc = baseline_by_algo.get(algo, 0)\n",
    "    adaptive_acc = adaptive_by_algo.get(algo, 0)\n",
    "    diff = adaptive_acc - baseline_acc\n",
    "    print(f\"{algo:<20} {baseline_acc:<10.3f} {adaptive_acc:<10.3f} {diff:<+10.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
