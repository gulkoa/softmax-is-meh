{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce55ac5",
   "metadata": {},
   "source": [
    "# Softmax is not enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f499ead3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\miniconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "MODEL_NAME = \"google/gemma-3-1b-pt\"\n",
    "# MODEL_NAME = \"google/gemma-3n-e2b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76d3ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polynomial_value(x: float, c: list[float]) -> float:\n",
    "    cur_val = 0\n",
    "    for i in range(len(c) - 1):\n",
    "        cur_val = (cur_val + c[i]) * x\n",
    "    return cur_val + c[-1]\n",
    "\n",
    "POLY_FIT = torch.tensor([-0.037, 0.481, -2.3, 4.917, -1.791]) # see Figure 6\n",
    "\n",
    "def softmax_adaptive_temperature(logits, dim, poly_fit=POLY_FIT, dtype=torch.float32):\n",
    "    \"\"\"\n",
    "    from \"Softmax is not Enough\" Figure 4, adapted to PyTorch\n",
    "    \"\"\"\n",
    "    original_probs = torch.softmax(logits, dim=dim, dtype=dtype)\n",
    "    entropy = torch.sum(-original_probs * torch.log(original_probs + 1e-9), axis=-1, keepdims=True) # compute the Shannon entropy\n",
    "    beta = torch.where(\n",
    "        entropy > 0.5,  # donâ€™t overcorrect low-entropy heads\n",
    "        torch.maximum(get_polynomial_value(entropy, poly_fit), torch.tensor(1.0)),  # never increase entropy\n",
    "        torch.tensor(1.0)\n",
    "    )\n",
    "    return torch.softmax(logits * beta, dim=dim, dtype=dtype)\n",
    "    \n",
    "class SoftmaxZero(nn.Module):\n",
    "    # for testing only\n",
    "    def __init__(self):\n",
    "        super(SoftmaxZero, self).__init__()\n",
    "\n",
    "    def forward(self, logits):\n",
    "        return torch.zeros_like(logits)\n",
    "    \n",
    "def adaptive_temperature_eager_attention_forward(\n",
    "    module: nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    dropout: float = 0.0,\n",
    "    scaling: Optional[float] = None,\n",
    "    softcap: Optional[float] = None,\n",
    "    **kwargs,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    A copy of gemma3's eager attention forward function, but using adaptive temperature softmax.\n",
    "    \"\"\"\n",
    "    if scaling is None:\n",
    "        scaling = module.head_dim**-0.5\n",
    "\n",
    "    # I don't like importing a foreign module like this, but ok for now\n",
    "    key_states = transformers.models.gemma3.modeling_gemma3.repeat_kv(key, module.num_key_value_groups)\n",
    "    value_states = transformers.models.gemma3.modeling_gemma3.repeat_kv(value, module.num_key_value_groups)\n",
    "\n",
    "    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n",
    "\n",
    "    if softcap is not None:\n",
    "        attn_weights = attn_weights / softcap\n",
    "        attn_weights = torch.tanh(attn_weights)\n",
    "        attn_weights = attn_weights * softcap\n",
    "    if attention_mask is not None:  # no matter the length, we just slice it\n",
    "        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "        attn_weights = attn_weights + causal_mask\n",
    "\n",
    "    # upcast attention to fp32\n",
    "    # attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n",
    "    attn_weights = softmax_adaptive_temperature(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n",
    "    \n",
    "    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n",
    "    attn_output = torch.matmul(attn_weights, value_states)\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    return attn_output, attn_weights\n",
    "\n",
    "# register the new attention function\n",
    "# this is so scrappy\n",
    "transformers.modeling_utils.ALL_ATTENTION_FUNCTIONS.register(\"adaptive_temperature_eager\", adaptive_temperature_eager_attention_forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3384f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.Gemma3ForCausalLM.from_pretrained(MODEL_NAME, attn_implementation=\"adaptive_temperature_eager\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dddf2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Ohio State University official mascot is <b>\"The Ohio State Buckeye\"</b>, the school's blue mascot. The school's mascot was first used in 1903 when the team won the National Championship. The mascot was designed by George H. Stover (1\n"
     ]
    }
   ],
   "source": [
    "pipe = transformers.pipeline(\"text-generation\", model=model, \n",
    "                tokenizer=tokenizer, device=\"cpu\")\n",
    "output = pipe(\"The Ohio State University official mascot is \", max_new_tokens=50)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df5dc0d",
   "metadata": {},
   "source": [
    "# CLRS Dataset Evaluation\n",
    "\n",
    "Now let's evaluate both models on algorithmic reasoning tasks from the CLRS dataset to see if adaptive temperature softmax improves performance on reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f04173e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import CLRS text utilities for generating algorithmic reasoning tasks\n",
    "import sys\n",
    "import os\n",
    "from datasets import IterableDataset, Value, Features\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), 'clrs-repo'))\n",
    "\n",
    "from clrs._src.clrs_text.huggingface_generators import clrs_generator\n",
    "\n",
    "\n",
    "ds = IterableDataset.from_generator(\n",
    "        clrs_generator,\n",
    "        features=Features(\n",
    "            {\n",
    "                \"text\": Value(dtype=\"string\", id=None),\n",
    "                \"question\": Value(dtype=\"string\", id=None),\n",
    "                \"answer\": Value(dtype=\"string\", id=None),\n",
    "                \"algo_name\": Value(dtype=\"string\", id=None),\n",
    "                \"length\": Value(dtype=\"int32\", id=None),\n",
    "                \"use_hints\": Value(dtype=\"bool_\", id=None),\n",
    "            }\n",
    "        ),\n",
    "        # gen_kwargs={\"algos_and_lengths\": algos_and_lengths},\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
