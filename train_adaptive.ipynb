{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b23cd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing clrs...\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'clrs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2612821736.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clrs-repo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Importing clrs...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mclrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclrs_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuggingface_generators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclrs_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Imports done.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'clrs'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# print(\"Importing torch...\")\n",
    "import torch\n",
    "from torch import nn\n",
    "# print(\"Importing transformers...\")\n",
    "import transformers\n",
    "# print(\"Importing peft...\")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "# print(\"Importing datasets...\")\n",
    "from datasets import IterableDataset, Features, Value\n",
    "import numpy as np\n",
    "\n",
    "# Add CLRS repo to path\n",
    "sys.path.append(os.path.join(os.getcwd(), 'clrs-repo'))\n",
    "print(\"Importing clrs...\")\n",
    "from clrs._src.clrs_text.huggingface_generators import clrs_generator\n",
    "print(\"Imports done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d090414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial coefficients (from paper)\n",
    "# We make this a Parameter so it can be learned\n",
    "POLY_FIT = torch.nn.Parameter(torch.tensor([-0.037, 0.481, -2.3, 4.917, -1.791]), requires_grad=True)\n",
    "\n",
    "def get_polynomial_value(x: torch.Tensor, c: torch.Tensor) -> torch.Tensor:\n",
    "    cur_val = torch.zeros_like(x)\n",
    "    # c is [a4, a3, a2, a1, a0] corresponding to x^4, x^3...\n",
    "    # Horner's method or simple loop\n",
    "    for i in range(len(c) - 1):\n",
    "        cur_val = (cur_val + c[i]) * x\n",
    "    return cur_val + c[-1]\n",
    "\n",
    "def softmax_adaptive_temperature(logits, dim, poly_fit, dtype=torch.float32):\n",
    "    \"\"\"\n",
    "    Adaptive temperature softmax.\n",
    "    \"\"\"\n",
    "    original_probs = torch.softmax(logits, dim=dim, dtype=dtype)\n",
    "    # Compute Shannon entropy\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    entropy = torch.sum(-original_probs * torch.log(original_probs + 1e-9), dim=-1, keepdim=True)\n",
    "    \n",
    "    # Calculate beta (inverse temperature)\n",
    "    # We use the poly_fit parameter passed in\n",
    "    poly_val = get_polynomial_value(entropy, poly_fit)\n",
    "    \n",
    "    # Apply guards\n",
    "    # 1. Low entropy guard: if H < 0.5, beta = 1.0\n",
    "    # 2. Dispersion guard: beta >= 1.0 (never increase entropy)\n",
    "    beta = torch.where(\n",
    "        entropy > 0.5,\n",
    "        torch.maximum(poly_val, torch.tensor(1.0, device=logits.device, dtype=logits.dtype)),\n",
    "        torch.tensor(1.0, device=logits.device, dtype=logits.dtype)\n",
    "    )\n",
    "    \n",
    "    return torch.softmax(logits * beta, dim=dim, dtype=dtype)\n",
    "\n",
    "def adaptive_temperature_eager_attention_forward(\n",
    "    module: nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    attention_mask: torch.Tensor = None,\n",
    "    dropout: float = 0.0,\n",
    "    scaling: float = None,\n",
    "    softcap: float = None,\n",
    "    **kwargs,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \n",
    "    if scaling is None:\n",
    "        scaling = module.head_dim**-0.5\n",
    "\n",
    "    # Access the learnable parameter\n",
    "    poly_fit_param = getattr(module, 'poly_fit', POLY_FIT)\n",
    "\n",
    "    # Standard Gemma attention logic\n",
    "    key_states = transformers.models.gemma3.modeling_gemma3.repeat_kv(key, module.num_key_value_groups)\n",
    "    value_states = transformers.models.gemma3.modeling_gemma3.repeat_kv(value, module.num_key_value_groups)\n",
    "\n",
    "    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n",
    "\n",
    "    if softcap is not None:\n",
    "        attn_weights = attn_weights / softcap\n",
    "        attn_weights = torch.tanh(attn_weights)\n",
    "        attn_weights = attn_weights * softcap\n",
    "    \n",
    "    if attention_mask is not None:\n",
    "        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "        attn_weights = attn_weights + causal_mask\n",
    "\n",
    "    # --- ADAPTIVE SOFTMAX ---\n",
    "    attn_weights = softmax_adaptive_temperature(attn_weights, dim=-1, poly_fit=poly_fit_param, dtype=torch.float32).to(query.dtype)\n",
    "    # ------------------------\n",
    "\n",
    "    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n",
    "    attn_output = torch.matmul(attn_weights, value_states)\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    \n",
    "    return attn_output, attn_weights\n",
    "\n",
    "# Register the function\n",
    "transformers.modeling_utils.ALL_ATTENTION_FUNCTIONS.register(\"adaptive_temperature_eager\", adaptive_temperature_eager_attention_forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39112ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering polynomial parameters...\n",
      "Setting up LoRA...\n",
      "trainable params: 745,477 || all params: 1,000,631,429 || trainable%: 0.0745\n",
      "Setting up dataset...\n",
      "Starting training...\n",
      "WARNING:tensorflow:From c:\\Users\\alexg\\miniconda3\\envs\\nlp\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL_NAME = \"google/gemma-3-1b-it\"\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = transformers.Gemma3ForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    attn_implementation=\"adaptive_temperature_eager\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Register POLY_FIT to all attention layers so it's part of the model\n",
    "print(\"Registering polynomial parameters...\")\n",
    "for name, module in model.named_modules():\n",
    "    if \"layers\" in name and \"self_attn\" in name:\n",
    "        # Register as a parameter of the module\n",
    "        # We share the same tensor across all layers\n",
    "        module.register_parameter('poly_fit', POLY_FIT)\n",
    "\n",
    "# LoRA Setup\n",
    "print(\"Setting up LoRA...\")\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# IMPORTANT: Unfreeze the polynomial parameter!\n",
    "# get_peft_model freezes all non-LoRA params.\n",
    "for name, param in model.named_parameters():\n",
    "    if \"poly_fit\" in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "# --- 3. Dataset Setup (CLRS) ---\n",
    "\n",
    "print(\"Setting up dataset...\")\n",
    "algos_and_lengths = {\n",
    "    \"minimum\": [16, 32] # Train on slightly longer sequences to encourage generalization? Or stick to short?\n",
    "    # Paper says they trained on simple task. Let's use reasonable lengths.\n",
    "}\n",
    "\n",
    "def generate_dataset():\n",
    "    ds = IterableDataset.from_generator(\n",
    "        clrs_generator,\n",
    "        features=Features({\n",
    "            \"text\": Value(\"string\"),\n",
    "            \"question\": Value(\"string\"),\n",
    "            \"answer\": Value(\"string\"),\n",
    "            \"algo_name\": Value(\"string\"),\n",
    "            \"length\": Value(\"int32\"),\n",
    "            \"use_hints\": Value(\"bool_\"),\n",
    "        }),\n",
    "        gen_kwargs={\n",
    "            \"algos_and_lengths\": algos_and_lengths,\n",
    "        },\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "dataset = generate_dataset()\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def collate_fn(examples):\n",
    "    # Prepare inputs for the model\n",
    "    # We want to train it to answer the question.\n",
    "    # Format: \"Question: <q>\\nAnswer: <a>\"\n",
    "    texts = []\n",
    "    for ex in examples:\n",
    "        # Construct a prompt. \n",
    "        # The CLRS generator outputs 'text' which might be the full trace, but 'question' and 'answer' are what we need.\n",
    "        # Let's use a standard chat format or simple completion.\n",
    "        # Gemma-it expects chat format usually, but for base completion:\n",
    "        prompt = f\"User: {ex['question']}\\nModel: {ex['answer']}\"\n",
    "        texts.append(prompt)\n",
    "    \n",
    "    encodings = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    encodings[\"labels\"] = encodings[\"input_ids\"].clone()\n",
    "    return encodings\n",
    "\n",
    "# --- 4. Training ---\n",
    "\n",
    "print(\"Starting training...\")\n",
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=100, # Short run for demonstration\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False, # Important for IterableDataset\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307927b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\miniconda3\\envs\\nlp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "WARNING:absl:Ignoring kwargs {'use_padding'} when building sampler class <class 'clrs._src.samplers.SortingSampler'>\n",
      "WARNING:absl:Sampling dataset on-the-fly, unlimited samples.\n",
      "WARNING:absl:Ignoring kwargs {'use_padding'} when building sampler class <class 'clrs._src.samplers.SortingSampler'>\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "print(\"Training complete.\")\n",
    "print(\"Final Polynomial Coefficients:\")\n",
    "print(POLY_FIT.data)\n",
    "\n",
    "# Save the model (LoRA adapters + the poly_fit param?)\n",
    "# LoRA save_pretrained only saves adapters.\n",
    "# We need to manually save the poly_fit if we want to reuse it.\n",
    "torch.save(POLY_FIT, \"./results/poly_fit.pt\")\n",
    "model.save_pretrained(\"./results/final_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
